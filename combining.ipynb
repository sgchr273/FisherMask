{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining gone fishing (BAIT) with Fisher mask (FISH)\n",
    "\n",
    "Fisher mask code in FISH folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.conv1.weight.grad\n",
    "type(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "* download the dataset cifar10, and create a dataloader  with appropriate preprocessing\n",
    "* iterate over minibatches from dataloader, calculate crossentropy loss, \n",
    "* in typical sgd training, we zero out the gradients before processing the next minibatch. this is done by calling model.zero_grad(), and then calling model.backward() to get the new gradients. Since we are not interested in training right now, we want the sum of gradients over all minibatches. so we should not call model.zero_grad()\n",
    "* identify the top 2% of all the weights in the model and make a new pk x k array, where p = 2% of the total number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import resnet\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#using the paper's model definition from resnet.py\n",
    "model = resnet.ResNet18()\n",
    "\n",
    "#arguments from run.py\n",
    "cifar_args = {'n_epoch': 3, 'transform': transforms.Compose([ \n",
    "                     transforms.RandomCrop(32, padding=4),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                 ]),\n",
    "                 'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                 'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                 'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                 'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "            }\n",
    "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_args['transform'])\n",
    "cifar_trainloader = torch.utils.data.DataLoader(cifar_trainset, batch_size=cifar_args['loader_tr_args']['batch_size'], num_workers=cifar_args['loader_tr_args']['num_workers'], shuffle=True)\n",
    "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_args['transform'])\n",
    "cifar_trainloader = torch.utils.data.DataLoader(cifar_trainset, batch_size=cifar_args['loader_te_args']['batch_size'], num_workers=cifar_args['loader_te_args']['num_workers'], shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=cifar_args['optimizer_args']['lr'], momentum=cifar_args['optimizer_args']['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.__contains__(\"weight\"):\n",
    "        print(name, param.grad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):#cifar_args['n_epoch']):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(cifar_trainloader):\n",
    "        inputs, labels = data\n",
    "        #we omit the step of zeroing gradients here\n",
    "        outputs, emb = model.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        #print(model.layer1[0].conv1.weight.grad) #how to access gradients of layers\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.__contains__(\"weight\"):\n",
    "                print(name, param.grad)\n",
    "print('Done Training.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eef878ddd0a8f23a70b3c8b2c776921e77b917de62b838cddf0368d96068c44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
