{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/home/sgchr/anaconda3/envs/pyt/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/local/home/sgchr/anaconda3/envs/pyt/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Using pretrained weights:\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "preprocess = ResNet18_Weights.DEFAULT.transforms()\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=preprocess)\n",
    "class_names = processed_trainset.classes\n",
    "processed_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=preprocess)\n",
    "dataloaders = {'train' : [], 'val' : []}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(processed_trainset, batch_size=100,\n",
    "                                                    shuffle=True, num_workers=4, pin_memory=True)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(processed_testset, batch_size=100,\n",
    "                                                    shuffle=False, num_workers=4, pin_memory=True)\n",
    "dataset_sizes = {'train' : len(processed_trainset), 'val' : len(processed_testset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 10)\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7]) False\n",
      "bn1.weight torch.Size([64]) False\n",
      "bn1.bias torch.Size([64]) False\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3]) False\n",
      "layer1.0.bn1.weight torch.Size([64]) False\n",
      "layer1.0.bn1.bias torch.Size([64]) False\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) False\n",
      "layer1.0.bn2.weight torch.Size([64]) False\n",
      "layer1.0.bn2.bias torch.Size([64]) False\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3]) False\n",
      "layer1.1.bn1.weight torch.Size([64]) False\n",
      "layer1.1.bn1.bias torch.Size([64]) False\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) False\n",
      "layer1.1.bn2.weight torch.Size([64]) False\n",
      "layer1.1.bn2.bias torch.Size([64]) False\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3]) False\n",
      "layer2.0.bn1.weight torch.Size([128]) False\n",
      "layer2.0.bn1.bias torch.Size([128]) False\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) False\n",
      "layer2.0.bn2.weight torch.Size([128]) False\n",
      "layer2.0.bn2.bias torch.Size([128]) False\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1]) False\n",
      "layer2.0.downsample.1.weight torch.Size([128]) False\n",
      "layer2.0.downsample.1.bias torch.Size([128]) False\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3]) False\n",
      "layer2.1.bn1.weight torch.Size([128]) False\n",
      "layer2.1.bn1.bias torch.Size([128]) False\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) False\n",
      "layer2.1.bn2.weight torch.Size([128]) False\n",
      "layer2.1.bn2.bias torch.Size([128]) False\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3]) False\n",
      "layer3.0.bn1.weight torch.Size([256]) False\n",
      "layer3.0.bn1.bias torch.Size([256]) False\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) False\n",
      "layer3.0.bn2.weight torch.Size([256]) False\n",
      "layer3.0.bn2.bias torch.Size([256]) False\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1]) False\n",
      "layer3.0.downsample.1.weight torch.Size([256]) False\n",
      "layer3.0.downsample.1.bias torch.Size([256]) False\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3]) False\n",
      "layer3.1.bn1.weight torch.Size([256]) False\n",
      "layer3.1.bn1.bias torch.Size([256]) False\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) False\n",
      "layer3.1.bn2.weight torch.Size([256]) False\n",
      "layer3.1.bn2.bias torch.Size([256]) False\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3]) False\n",
      "layer4.0.bn1.weight torch.Size([512]) False\n",
      "layer4.0.bn1.bias torch.Size([512]) False\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) False\n",
      "layer4.0.bn2.weight torch.Size([512]) False\n",
      "layer4.0.bn2.bias torch.Size([512]) False\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1]) False\n",
      "layer4.0.downsample.1.weight torch.Size([512]) False\n",
      "layer4.0.downsample.1.bias torch.Size([512]) False\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3]) False\n",
      "layer4.1.bn1.weight torch.Size([512]) False\n",
      "layer4.1.bn1.bias torch.Size([512]) False\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) False\n",
      "layer4.1.bn2.weight torch.Size([512]) False\n",
      "layer4.1.bn2.bias torch.Size([512]) False\n",
      "fc.weight torch.Size([10, 512]) True\n",
      "fc.bias torch.Size([10]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "dataset_sizes = {'train' : 50000 , 'val' : 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet18_feats_cifar10.pyt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.cpu()\n",
    "                labels = labels.cpu()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv, hist = train_model(model, criterion, optimizer,\n",
    "                         exp_lr_scheduler, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "parameters = tuple(model.parameters())\n",
    "sq_grads_expect = {i: np.zeros(p.shape) for i, p in enumerate(parameters)} # \n",
    "\n",
    "for test_batch, test_labels in dataloaders['val']:\n",
    "    model.zero_grad()\n",
    "    outputs = model(test_batch)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    print(torch.sum(preds == test_labels.data) / len(test_labels))\n",
    "    \n",
    "    probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list = torch.autograd.grad(log_probs[n][c], parameters, retain_graph=True)\n",
    "            for i, grad in enumerate(grad_list):    # different layers\n",
    "                gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                sq_grads_expect[i] += gsq.detach().numpy() # sq_grads_expect[i] + gsq\n",
    "                del gsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sq_grads_expect, open(\"save.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_t = list(sq_grads_expect.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_arrays = np.hstack([t.flatten() for t in sq_grads_expect.values()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lengths = [len(ten.flatten()) for ten in list_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_lengths = np.cumsum(list_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idxs = np.argsort(combined_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top = int(0.02 * len(combined_arrays))\n",
    "top_idxs = sorted_idxs[-num_top:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_num = [[] for i in range(len(list_t))]\n",
    "for idx in top_idxs:\n",
    "    prev_length = 0\n",
    "    for idx_layer_num, length in enumerate(cum_lengths):\n",
    "        if idx < length and length > prev_length: \n",
    "            # print(len_idx)\n",
    "            try:\n",
    "                # s_num[len_idx].append(np.where(combined_s[idx] == list_s[len_idx])[0][0])\n",
    "                idx_tuple = np.nonzero(combined_arrays[idx] == list_t[idx_layer_num])\n",
    "                '''pass only numpy or python objects to numpy functions'''\n",
    "                # s_num[len_idx].append([idx[0] for idx in idx_tuple])\n",
    "                t_num[idx_layer_num].append(idx_tuple)\n",
    "            except IndexError:\n",
    "                print(\"caught error: \", idx, idx_layer_num, length, t_num)\n",
    "                break\n",
    "        prev_length = length\n",
    "print(t_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot([len(t) for t in t_num]) # gets biased by number of weights in a layer\n",
    "plt.plot([len(t) / len(g.flatten()) for t, g in zip(t_num, list_t)], 'o')\n",
    "plt.xlabel('layer number')\n",
    "plt.ylabel('fraction of weights considered important in a layer')\n",
    "plt.title('threshold for important weights is 0.02')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_layer_nums = np.nonzero([len(t) / len(g.flatten()) > 0.5 for t, g in zip(t_num, list_t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_layer_nums[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = tuple(model.named_parameters())\n",
    "# parameters[-2][0]\n",
    "[parameters[num][0] for num in important_layer_nums[0]] # 47 important layers\n",
    "# [parameters[num][0] for num in range(62) if num not in important_layer_nums[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cafaace53dbfa2f2f80725cf8b8a707efc1de97ef1c6af3883bc118682f6610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
