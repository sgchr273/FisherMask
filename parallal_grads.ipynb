{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import resnet\n",
    "from dataset import get_dataset, get_handler\n",
    "from torchvision import transforms\n",
    "from operator import add\n",
    "\n",
    "outputs = (torch.rand(100,4, requires_grad=True ))\n",
    "net = resnet.ResNet18(num_classes=4)\n",
    "parameters = tuple(net.parameters())\n",
    "probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "log_probs = F.log_softmax(outputs, dim=1)\n",
    "N, C = log_probs.shape\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape) for i, p in enumerate(parameters)}\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')\n",
    "dim = np.shape(X_tr)[1:]\n",
    "handler = get_handler('CIFAR10')\n",
    "args = {'MNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'FashionMNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'SVHN':\n",
    "                {'n_epoch': 20, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'CIFAR10':\n",
    "                {'n_epoch': 3, 'transform': transforms.Compose([ \n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                ]),\n",
    "                'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 100, 'num_workers': 1}, # change back to 1000\n",
    "                'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])}\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:  torch.Size([100, 4])\n",
      "Outputs:  torch.Size([100, 4])\n",
      "Outputs:  torch.Size([100, 4])\n",
      "Outputs:  torch.Size([100, 4])\n",
      "Outputs:  torch.Size([100, 4])\n",
      "Outputs:  torch.Size([100, 4])\n"
     ]
    }
   ],
   "source": [
    "#Modified code\n",
    "test_loader = DataLoader(handler(X_tr, Y_tr, transform=args['CIFAR10']['transform']), shuffle=False, **args['CIFAR10']['loader_te_args'])\n",
    "num_samples = 3 # used by FISH mask paper\n",
    "idx = 0\n",
    "sq_grads_expect = []\n",
    "\n",
    "for test_batch, test_labels, idxs in test_loader:\n",
    "    test_batch, test_labels = test_batch, test_labels\n",
    "    outputs, e1 = net(test_batch)\n",
    "    print('Outputs: ',outputs.shape)\n",
    "    _, preds = torch.max(outputs, 1)         \n",
    "    probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    grad_outs = torch.zeros_like(log_probs)\n",
    "    for c in range(len(preds)):\n",
    "        grad_outs[c, preds[c]] = 1\n",
    "    grad_list = torch.autograd.grad(log_probs*torch.sqrt(probs), parameters, grad_outputs=grad_outs, retain_graph=True)\n",
    "    grad_list = [np.array(torch.square(i)) for i in grad_list]\n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )\n",
    "    net.zero_grad()\n",
    "\n",
    "    print('Outputs: ',outputs.shape)\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(log_probs[n][c], parameters, retain_graph=True)\n",
    "            for i, grad in enumerate(grad_list_orig):    # different layers\n",
    "                gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy() # sq_grads_expect[i] + gsq\n",
    "                del gsq\n",
    "            net.zero_grad()\n",
    "\n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing gradients for the vector valued function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2., 2., 2., 2.]), tensor([0., 0., 0., 0.]), tensor([3.2179, 5.4168, 3.4557, 5.7125]))\n",
      "(tensor([3.3963, 1.6956, 2.2514, 2.3443]), tensor([1.4453, 1.6891, 0.4078, 0.1368]), tensor([2.5065, 0.2205, 0.9550, 0.3789]))\n",
      "(tensor([2., 2., 2., 2.]), None, tensor([3.2179, 5.4168, 3.4557, 5.7125]))\n",
      "(tensor([3.3963, 1.6956, 2.2514, 2.3443]), tensor([1.4453, 1.6891, 0.4078, 0.1368]), tensor([2.5065, 0.2205, 0.9550, 0.3789]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "n = 4\n",
    "x = torch.rand(n, requires_grad = True)\n",
    "y = torch.rand(n, requires_grad = True)\n",
    "z = torch.rand(n, requires_grad = True)\n",
    "f = torch.column_stack([2 * x + 3 * torch.square(z), torch.square(y) + 4 * z * torch.square(x)])\n",
    "f1 = 2 * x + 3 * torch.square(z)\n",
    "f2 = torch.square(y) + 4 * z * torch.square(x)\n",
    "\n",
    "for c in range(f.shape[1]):\n",
    "    grad_outs = torch.zeros_like(f)\n",
    "    grad_outs[:, c] = 1\n",
    "    print(torch.autograd.grad(f, (x, y, z), grad_outputs=grad_outs, retain_graph=True))\n",
    "\n",
    "# verifying calculated gradients are correct\n",
    "#print(x, y, z)\n",
    "print(torch.autograd.grad(f1, (x, y, z), grad_outputs=torch.ones_like(f1), allow_unused=True))\n",
    "print(torch.autograd.grad(f2, (x, y, z), grad_outputs=torch.ones_like(f2), allow_unused=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
