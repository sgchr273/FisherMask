{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import resnet\n",
    "from dataset import get_dataset, get_handler\n",
    "from torchvision import transforms\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add5 = lambda x : x + 5\n",
    "add1 = lambda x : x + 1\n",
    "add10 = lambda x : x + 10\n",
    "\n",
    "net = resnet.ResNet18(num_classes=4)\n",
    "\n",
    "args = {'MNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'FashionMNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'SVHN':\n",
    "                {'n_epoch': 20, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'CIFAR10':\n",
    "                {'n_epoch': 3, 'transform': transforms.Compose([ \n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                ]),\n",
    "                'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 10, 'num_workers': 1}, # change back to 1000\n",
    "                'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])}\n",
    "                }\n",
    "parameters = tuple(net.parameters())\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')\n",
    "dim = np.shape(X_tr)[1:]\n",
    "handler = get_handler('CIFAR10')\n",
    "test_loader = DataLoader(handler(X_tr, Y_tr, transform=args['CIFAR10']['transform']), shuffle=False, **args['CIFAR10']['loader_te_args'])\n",
    "num_samples = 1 # used by FISH mask paper\n",
    "idx = 0\n",
    "sq_grads_expect = []\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape,dtype=np.float32) for i, p in enumerate(parameters)}\n",
    "parameters = tuple(net.parameters())\n",
    "\n",
    "for test_batch, test_labels, idxs in test_loader:\n",
    " \n",
    "    #Method 1\n",
    "    # test_batch, test_labels = test_batch, test_labels\n",
    "    outputs, e1 = net(test_batch)\n",
    "    print('Outputs: ',outputs.shape)\n",
    "    # _, preds = torch.max(outputs, 1)         \n",
    "    probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    ps = probs.detach()\n",
    "    f = log_probs #torch.sqrt(ps)*log_probs\n",
    "    grad_list = []\n",
    "    for c in range(C):\n",
    "        grad_outs = torch.zeros_like(f)\n",
    "        grad_outs[:, c] = 1\n",
    "        grad = torch.autograd.grad(f, parameters, grad_outputs=grad_outs, retain_graph=True) #f\n",
    "        print(grad[0].shape)\n",
    "        #Below if-else used to add tuples element-wise\n",
    "        # grad = torch.square(grad)\n",
    "        \n",
    "        if c == 0:\n",
    "            grad_list = [(add10(g)/N).detach().numpy() for g in grad] #torch.square(g)/N\n",
    "        else:\n",
    "            grad_list = list( map(add, grad_list,  [(add10(g)/N).detach().numpy() for g in grad]) ) #torch.square(g)/N\n",
    "            \n",
    "        net.zero_grad()\n",
    "    \n",
    "    # grad_list = [np.array(i/N) for i in grad_list]\n",
    "    #Below if-else used to add tuples element-wise\n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )\n",
    "        \n",
    "    #Method 2\n",
    "    print(\"original way next\")\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(f[n][c], parameters, retain_graph=True) #log_probs\n",
    "            for i, grad in enumerate(grad_list_orig):   # different layers\n",
    "                if (n ==0) & (c==0) & (i ==0):\n",
    "                    print(grad.shape)\n",
    "                # gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                gsq = add1(grad).to('cpu') / N # torch.square(grad)\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy() # sq_grads_expect[i] + gsq\n",
    "                # del grad\n",
    "            net.zero_grad()\n",
    "    \n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break\n",
    "\n",
    "print(sq_grads_expect[0])\n",
    "print(sq_grads_expect_orig[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "original way next\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JoshC\\Desktop\\active-nn\\parallal_grads-2.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoshC/Desktop/active-nn/parallal_grads-2.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m grads_arr \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoshC/Desktop/active-nn/parallal_grads-2.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(C):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JoshC/Desktop/active-nn/parallal_grads-2.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     ft_compute_grad \u001b[39m=\u001b[39m grad(compute_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoshC/Desktop/active-nn/parallal_grads-2.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     ft_compute_sample_grad \u001b[39m=\u001b[39m vmap(ft_compute_grad, in_dims\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JoshC/Desktop/active-nn/parallal_grads-2.ipynb#W2sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     ft_per_sample_grads \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(ft_compute_sample_grad(params, buffers, train_batch, c))\u001b[39m#train_labels))\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import resnet\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.func import functional_call, vmap, grad\n",
    "\n",
    "net = resnet.ResNet18(num_classes=10)\n",
    "args = {\n",
    "            'CIFAR10':\n",
    "                {\n",
    "                    'n_epoch': 3, \n",
    "                    'transform': transforms.Compose([ \n",
    "                        transforms.RandomCrop(32, padding=4),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]),\n",
    "                    'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                    'loader_te_args':{'batch_size': 10, 'num_workers': 1}, # change back to 1000\n",
    "                    'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                    'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "                }\n",
    "        }\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=args['CIFAR10']['transform'])\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=args['CIFAR10']['loader_tr_args']['batch_size'], shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=args['CIFAR10']['transform'])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=args['CIFAR10']['loader_te_args']['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "def compute_loss(params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    #targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = functional_call(net, (params, buffers), (batch,))\n",
    "    loss = loss_fn(predictions[0], target)\n",
    "    return loss\n",
    "            \n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return torch.from_numpy(predictions[:, targets]) #F.nll_loss(predictions, targets)\n",
    "\n",
    "\n",
    "parameters = tuple(net.parameters())\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape,dtype=np.float32) for i, p in enumerate(parameters)}\n",
    "\n",
    "num_samples=1\n",
    "idx=0\n",
    "\n",
    "#params = {k: v.detach() for k, v in net.named_parameters()}\n",
    "#buffers = {k: v.detach() for k, v in net.named_buffers()}\n",
    "#ft_compute_grad = grad(compute_loss)\n",
    "#ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "#ft_per_sample_grads = ft_compute_sample_grad(params, buffers, torch.permute(torch.from_numpy(trainset.data),(0,3,1,2)).float(), torch.tensor(trainset.targets))\n",
    "\n",
    "for train_batch, train_labels in train_loader:\n",
    "    outputs, e1 = net(train_batch)      \n",
    "    probs = F.softmax(outputs, dim=1)#.to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    print(\"original way next\")\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(log_probs[n][c], parameters, retain_graph=True)\n",
    "            for i, grad in enumerate(grad_list_orig):   # different layers\n",
    "                # gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                gsq = grad#.to('cpu') # torch.square(grad)\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy()\n",
    "                # del grad\n",
    "            net.zero_grad()\n",
    "\n",
    "    params = {k: v.detach() for k, v in net.named_parameters()}\n",
    "    buffers = {k: v.detach() for k, v in net.named_buffers()}\n",
    "    grads_arr = []\n",
    "    for c in range(C):\n",
    "        ft_compute_grad = grad(compute_loss)\n",
    "        ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "        ft_per_sample_grads = sum(ft_compute_sample_grad(params, buffers, train_batch, c))#train_labels))\n",
    "        grads_arr.append(ft_per_sample_grads)\n",
    "        net.zero_grad()\n",
    "    \n",
    "    grads_arr = sum(grads_arr)\n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[12.704348  , 12.60496   ,  7.723358  ],\n",
       "        [ 9.278099  ,  9.066983  ,  3.3033621 ],\n",
       "        [ 2.2450042 ,  2.668539  , -3.1830425 ]],\n",
       "\n",
       "       [[10.8174    , 11.973187  ,  8.793834  ],\n",
       "        [ 6.7906795 ,  6.8204546 ,  3.349778  ],\n",
       "        [ 0.9677659 ,  2.1111672 , -2.3996966 ]],\n",
       "\n",
       "       [[12.784865  , 14.335051  , 11.729529  ],\n",
       "        [ 9.406301  ,  9.912758  ,  7.700953  ],\n",
       "        [ 2.698927  ,  4.172598  ,  0.61233765]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_grads_expect_orig[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.3205,  3.0951,  6.5645],\n",
       "         [ 2.6982,  1.3491,  0.6842],\n",
       "         [ 1.7819,  4.7311,  1.5717]],\n",
       "\n",
       "        [[ 2.2331,  2.5682,  6.2995],\n",
       "         [ 1.3808,  1.3336,  0.0521],\n",
       "         [ 0.5188,  4.1565, -0.4611]],\n",
       "\n",
       "        [[ 2.0457,  2.7036,  6.1600],\n",
       "         [ 0.5252,  0.5636, -0.4522],\n",
       "         [ 0.2869,  3.7947, -0.5545]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ft_per_sample_grads['conv1.weight'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for (i, j) in net.named_parameters()])\n",
    "print(sq_grads_expect_orig[0].shape)\n",
    "print(sum(ft_per_sample_grads[\"conv1.weight\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sq_grads_expect_orig[0][0])\n",
    "print(sum(ft_per_sample_grads[\"conv1.weight\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all([torch.allclose(sq_grads_expect[i], torch.from_numpy(sq_grads_expect_orig[i]).float(), atol=1e-4) for i in range(61)]) \n",
    "all([np.allclose(sq_grads_expect[i], sq_grads_expect_orig[i], atol=1e-4) for i in range(61)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_grads_expect = []\n",
    "sq_grads = {i: np.zeros(4,dtype=np.float32) for i in range(3)} # 4 is the n and 3 is the number of parameters\n",
    "\n",
    "for idx in range(2):\n",
    "    n = 4\n",
    "    x = torch.rand(n, requires_grad = True)\n",
    "    y = torch.rand(n, requires_grad = True)\n",
    "    z = torch.rand(n, requires_grad = True)\n",
    "    # sq_grads = {i: np.zeros(p.shape) for i, p in enumerate((x,y,z))}\n",
    "    f = torch.column_stack([2 * x + 3 * torch.square(z), torch.square(y) + 4 * z * torch.square(x)])\n",
    "    probs = torch.abs(torch.rand(f.shape, requires_grad=False))\n",
    "    pf = torch.sqrt(probs) * f\n",
    "    N, C = f.shape\n",
    "    grad_list = []\n",
    "    for c in range(C):\n",
    "        grad_outs = torch.zeros_like(f)\n",
    "        grad_outs[:, c] = 1\n",
    "        n_grad = torch.autograd.grad(pf, (x, y, z), grad_outputs=grad_outs, retain_graph=True)\n",
    "        if c == 0:\n",
    "            grad_list = [(torch.square(g)/N).detach().numpy() for g in n_grad] # #n_grad\n",
    "        else:\n",
    "            grad_list = list( map(add, grad_list,  [(torch.square(g)/N).detach().numpy() for g in n_grad]) )\n",
    "        \n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )        \n",
    "\n",
    "    # print(grad_list)\n",
    "    # print(f.grad)\n",
    "    # verifying calculated gradients are correct\n",
    "    # print(x, y, z)\n",
    "\n",
    "    for idx in range(N):\n",
    "        for c in range(C):\n",
    "            new_grad = torch.autograd.grad(f[idx][c], (x, y, z),  retain_graph=True)\n",
    "            # print(new_grad)\n",
    "            for i, grad in enumerate(new_grad):\n",
    "                # print(type(grad), grad.shape)\n",
    "                gsq = torch.square(grad).to('cpu') * probs[idx][c]  / N\n",
    "                sq_grads[i] += gsq.detach().numpy()\n",
    "\n",
    "print(sq_grads_expect)\n",
    "print(sq_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sq_grads_expect[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sq_grads[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet.ResNet18(num_classes=4)\n",
    "args = {'MNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'FashionMNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'SVHN':\n",
    "                {'n_epoch': 20, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'CIFAR10':\n",
    "                {'n_epoch': 3, 'transform': transforms.Compose([ \n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                ]),\n",
    "                'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 10, 'num_workers': 1}, # change back to 1000\n",
    "                'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])}\n",
    "                }\n",
    "parameters = tuple(net.parameters())\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')\n",
    "dim = np.shape(X_tr)[1:]\n",
    "handler = get_handler('CIFAR10')\n",
    "test_loader = DataLoader(handler(X_tr, Y_tr, transform=args['CIFAR10']['transform']), shuffle=False, **args['CIFAR10']['loader_te_args'])\n",
    "num_samples = 2 # used by FISH mask paper\n",
    "idx = 0\n",
    "sq_grads_expect = []\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape) for i, p in enumerate(parameters)}\n",
    "parameters = tuple(net.parameters())\n",
    "\n",
    "for test_batch, test_labels, idxs in test_loader:\n",
    " \n",
    "    \n",
    "    test_batch, test_labels = test_batch, test_labels\n",
    "    outputs, e1 = net(test_batch)\n",
    "    print('Outputs: ',outputs.shape)\n",
    "    _, preds = torch.max(outputs, 1)         \n",
    "    probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    ps = probs.detach()\n",
    "    f = log_probs*torch.sqrt(ps)\n",
    "\n",
    "    for c in range(f.shape[1]):\n",
    "        grad_outs = torch.zeros_like(f)\n",
    "        grad_outs[:, c] = 1\n",
    "        grad = torch.autograd.grad(f, parameters, grad_outputs=grad_outs, retain_graph=True)\n",
    "        #Below if-else used to add tuples element-wise\n",
    "        if c == 0:\n",
    "            grad_list = [torch.square(g) for g in grad]\n",
    "        else:\n",
    "            grad_list = list( map(add, grad_list, [torch.square(g) for g in grad]) )\n",
    "        \n",
    "        net.zero_grad()\n",
    "    \n",
    "    grad_list = [np.array(i/N) for i in grad_list]\n",
    "    #Below if-else used to add tuples element-wise\n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )\n",
    "\n",
    "    print(\"original way next\")\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(log_probs[n][c], parameters, retain_graph=True)\n",
    "            for i, grad in enumerate(grad_list_orig):    # different layers\n",
    "                gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy() # sq_grads_expect[i] + gsq\n",
    "                del gsq\n",
    "            net.zero_grad()\n",
    "    \n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet.ResNet18(num_classes=4)\n",
    "parameters = tuple(net.parameters())\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel, params, buffers = make_functional_with_buffers(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PER-SAMPLE-GRADIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a simple CNN and loss function:\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "num_models = 10\n",
    "batch_size = 64\n",
    "data = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "\n",
    "targets = torch.randint(10, (64,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device=device)\n",
    "predictions = model(data) # move the entire mini-batch through the model\n",
    "\n",
    "loss = loss_fn(predictions, targets)\n",
    "loss.backward() # back propogate the 'average' gradient of this mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(sample, target):\n",
    "    \n",
    "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
    "    target = target.unsqueeze(0)\n",
    "\n",
    "    prediction = model(sample)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    return torch.autograd.grad(loss, list(model.parameters()))\n",
    "\n",
    "\n",
    "def compute_sample_grads(data, targets):\n",
    "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
    "    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n",
    "    sample_grads = zip(*sample_grads)\n",
    "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
    "    return sample_grads\n",
    "\n",
    "per_sample_grads = compute_sample_grads(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(per_sample_grads[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "fmodel, params, buffers = make_functional_with_buffers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_grad = grad(compute_loss_stateless_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n",
    "\n",
    "# we can double check that the results using functorch grad and vmap match the results of hand processing each one individually:\n",
    "for per_sample_grad, ft_per_sample_grad in zip(per_sample_grads, ft_per_sample_grads):\n",
    "    assert torch.allclose(per_sample_grad, ft_per_sample_grad, atol=3e-3, rtol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
