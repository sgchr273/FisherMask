{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/home/sgchr/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import resnet\n",
    "from dataset import get_dataset, get_handler\n",
    "from torchvision import transforms\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add5 = lambda x : x + 5\n",
    "add1 = lambda x : x + 1\n",
    "add10 = lambda x : x + 10\n",
    "\n",
    "net = resnet.ResNet18(num_classes=4)\n",
    "\n",
    "args = {'MNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'FashionMNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'SVHN':\n",
    "                {'n_epoch': 20, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'CIFAR10':\n",
    "                {'n_epoch': 3, 'transform': transforms.Compose([ \n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                ]),\n",
    "                'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 10, 'num_workers': 1}, # change back to 1000\n",
    "                'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])}\n",
    "                }\n",
    "parameters = tuple(net.parameters())\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')\n",
    "dim = np.shape(X_tr)[1:]\n",
    "handler = get_handler('CIFAR10')\n",
    "test_loader = DataLoader(handler(X_tr, Y_tr, transform=args['CIFAR10']['transform']), shuffle=False, **args['CIFAR10']['loader_te_args'])\n",
    "num_samples = 1 # used by FISH mask paper\n",
    "idx = 0\n",
    "sq_grads_expect = []\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape,dtype=np.float32) for i, p in enumerate(parameters)}\n",
    "parameters = tuple(net.parameters())\n",
    "\n",
    "for test_batch, test_labels, idxs in test_loader:\n",
    " \n",
    "    #Method 1\n",
    "    # test_batch, test_labels = test_batch, test_labels\n",
    "    outputs, e1 = net(test_batch)\n",
    "    print('Outputs: ',outputs.shape)\n",
    "    # _, preds = torch.max(outputs, 1)         \n",
    "    probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    ps = probs.detach()\n",
    "    f = log_probs #torch.sqrt(ps)*log_probs\n",
    "    grad_list = []\n",
    "    for c in range(C):\n",
    "        grad_outs = torch.zeros_like(f)\n",
    "        grad_outs[:, c] = 1\n",
    "        grad = torch.autograd.grad(f, parameters, grad_outputs=grad_outs, retain_graph=True) #f\n",
    "        print(grad[0].shape)\n",
    "        #Below if-else used to add tuples element-wise\n",
    "        # grad = torch.square(grad)\n",
    "        \n",
    "        if c == 0:\n",
    "            grad_list = [(add10(g)/N).detach().numpy() for g in grad] #torch.square(g)/N\n",
    "        else:\n",
    "            grad_list = list( map(add, grad_list,  [(add10(g)/N).detach().numpy() for g in grad]) ) #torch.square(g)/N\n",
    "            \n",
    "        net.zero_grad()\n",
    "    \n",
    "    # grad_list = [np.array(i/N) for i in grad_list]\n",
    "    #Below if-else used to add tuples element-wise\n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )\n",
    "        \n",
    "    #Method 2\n",
    "    print(\"original way next\")\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(f[n][c], parameters, retain_graph=True) #log_probs\n",
    "            for i, grad in enumerate(grad_list_orig):   # different layers\n",
    "                if (n ==0) & (c==0) & (i ==0):\n",
    "                    print(grad.shape)\n",
    "                # gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                gsq = add1(grad).to('cpu') / N # torch.square(grad)\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy() # sq_grads_expect[i] + gsq\n",
    "                # del grad\n",
    "            net.zero_grad()\n",
    "    \n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break\n",
    "\n",
    "print(sq_grads_expect[0])\n",
    "print(sq_grads_expect_orig[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunc\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_call, vmap, grad\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m net \u001b[39m=\u001b[39m resnet\u001b[39m.\u001b[39mResNet18(num_classes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCIFAR10\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                 {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                 }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/active_nn/parallal_grads-2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         }\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.func'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import resnet\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.func import functional_call, vmap, grad\n",
    "\n",
    "net = resnet.ResNet18(num_classes=10)\n",
    "args = {\n",
    "            'CIFAR10':\n",
    "                {\n",
    "                    'n_epoch': 3, \n",
    "                    'transform': transforms.Compose([ \n",
    "                        transforms.RandomCrop(32, padding=4),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]),\n",
    "                    'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                    'loader_te_args':{'batch_size': 10, 'num_workers': 1}, # change back to 1000\n",
    "                    'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                    'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "                }\n",
    "        }\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=args['CIFAR10']['transform'])\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=args['CIFAR10']['loader_tr_args']['batch_size'], shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=args['CIFAR10']['transform'])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=args['CIFAR10']['loader_te_args']['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "c_global = -1\n",
    "\n",
    "def compute_loss(params, buffers, sample, target):\n",
    "    global c_global\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = functional_call(net, (params, buffers), (batch,))\n",
    "    #return predictions[:, c_global] # gives a tensor not callable error\n",
    "    loss = loss_fn(predictions[0], targets)\n",
    "    return loss\n",
    "            \n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)\n",
    "\n",
    "\n",
    "parameters = tuple(net.parameters())\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape,dtype=np.float32) for i, p in enumerate(parameters)}\n",
    "\n",
    "num_samples=1\n",
    "idx=0\n",
    "\n",
    "#params = {k: v.detach() for k, v in net.named_parameters()}\n",
    "#buffers = {k: v.detach() for k, v in net.named_buffers()}\n",
    "#ft_compute_grad = grad(compute_loss)\n",
    "#ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "#ft_per_sample_grads = ft_compute_sample_grad(params, buffers, torch.permute(torch.from_numpy(trainset.data),(0,3,1,2)).float(), torch.tensor(trainset.targets))\n",
    "\n",
    "for train_batch, train_labels in train_loader:\n",
    "    outputs, e1 = net(train_batch)      \n",
    "    probs = F.softmax(outputs, dim=1)#.to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(log_probs[n][c], parameters, retain_graph=True)\n",
    "            for i, grad in enumerate(grad_list_orig):   # different layers\n",
    "                # gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                gsq = grad#.to('cpu') # torch.square(grad)\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy()\n",
    "                # del grad\n",
    "            net.zero_grad()\n",
    "\n",
    "    print(\"Beginning method 2\")\n",
    "    params = {k: v.detach() for k, v in net.named_parameters()}\n",
    "    buffers = {k: v.detach() for k, v in net.named_buffers()}\n",
    "    grads_arr = []\n",
    "    for c in range(C):\n",
    "        c_global = c\n",
    "        ft_compute_grad = grad(compute_loss)\n",
    "        ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "        ft_per_sample_grads = ft_compute_sample_grad(params, buffers, train_batch, train_labels)\n",
    "        grads_arr.append(list(ft_per_sample_grads.values()))\n",
    "        net.zero_grad()\n",
    "    \n",
    "    grads_arr = [sum(x) for x in zip(*grads_arr)]\n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3], [2,3,1])\n",
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = [[torch.tensor([1,2,3]),torch.tensor([1,2,3])],[torch.tensor([2,3,1]),torch.tensor([1,2,3])],[torch.tensor([3,4,6]),torch.tensor([1,2,3])]]\n",
    "[sum(x) for x in zip(*ar)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_grads_expect_orig[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ft_per_sample_grads['conv1.weight'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for (i, j) in net.named_parameters()])\n",
    "print(sq_grads_expect_orig[0].shape)\n",
    "print(sum(ft_per_sample_grads[\"conv1.weight\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sq_grads_expect_orig[0][0])\n",
    "print(sum(ft_per_sample_grads[\"conv1.weight\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all([torch.allclose(sq_grads_expect[i], torch.from_numpy(sq_grads_expect_orig[i]).float(), atol=1e-4) for i in range(61)]) \n",
    "all([np.allclose(sq_grads_expect[i], sq_grads_expect_orig[i], atol=1e-4) for i in range(61)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_grads_expect = []\n",
    "sq_grads = {i: np.zeros(4,dtype=np.float32) for i in range(3)} # 4 is the n and 3 is the number of parameters\n",
    "\n",
    "for idx in range(2):\n",
    "    n = 4\n",
    "    x = torch.rand(n, requires_grad = True)\n",
    "    y = torch.rand(n, requires_grad = True)\n",
    "    z = torch.rand(n, requires_grad = True)\n",
    "    # sq_grads = {i: np.zeros(p.shape) for i, p in enumerate((x,y,z))}\n",
    "    f = torch.column_stack([2 * x + 3 * torch.square(z), torch.square(y) + 4 * z * torch.square(x)])\n",
    "    probs = torch.abs(torch.rand(f.shape, requires_grad=False))\n",
    "    pf = torch.sqrt(probs) * f\n",
    "    N, C = f.shape\n",
    "    grad_list = []\n",
    "    for c in range(C):\n",
    "        grad_outs = torch.zeros_like(f)\n",
    "        grad_outs[:, c] = 1\n",
    "        n_grad = torch.autograd.grad(pf, (x, y, z), grad_outputs=grad_outs, retain_graph=True)\n",
    "        if c == 0:\n",
    "            grad_list = [(torch.square(g)/N).detach().numpy() for g in n_grad] # #n_grad\n",
    "        else:\n",
    "            grad_list = list( map(add, grad_list,  [(torch.square(g)/N).detach().numpy() for g in n_grad]) )\n",
    "        \n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )        \n",
    "\n",
    "    # print(grad_list)\n",
    "    # print(f.grad)\n",
    "    # verifying calculated gradients are correct\n",
    "    # print(x, y, z)\n",
    "\n",
    "    for idx in range(N):\n",
    "        for c in range(C):\n",
    "            new_grad = torch.autograd.grad(f[idx][c], (x, y, z),  retain_graph=True)\n",
    "            # print(new_grad)\n",
    "            for i, grad in enumerate(new_grad):\n",
    "                # print(type(grad), grad.shape)\n",
    "                gsq = torch.square(grad).to('cpu') * probs[idx][c]  / N\n",
    "                sq_grads[i] += gsq.detach().numpy()\n",
    "\n",
    "print(sq_grads_expect)\n",
    "print(sq_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sq_grads_expect[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sq_grads[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet.ResNet18(num_classes=4)\n",
    "args = {'MNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'FashionMNIST':\n",
    "                {'n_epoch': 10, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'SVHN':\n",
    "                {'n_epoch': 20, 'transform': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))]),\n",
    "                'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 1000, 'num_workers': 1},\n",
    "                'optimizer_args':{'lr': 0.01, 'momentum': 0.5}},\n",
    "            'CIFAR10':\n",
    "                {'n_epoch': 3, 'transform': transforms.Compose([ \n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                ]),\n",
    "                'loader_tr_args':{'batch_size': 128, 'num_workers': 1},\n",
    "                'loader_te_args':{'batch_size': 10, 'num_workers': 1}, # change back to 1000\n",
    "                'optimizer_args':{'lr': 0.05, 'momentum': 0.3},\n",
    "                'transformTest': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])}\n",
    "                }\n",
    "parameters = tuple(net.parameters())\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')\n",
    "dim = np.shape(X_tr)[1:]\n",
    "handler = get_handler('CIFAR10')\n",
    "test_loader = DataLoader(handler(X_tr, Y_tr, transform=args['CIFAR10']['transform']), shuffle=False, **args['CIFAR10']['loader_te_args'])\n",
    "num_samples = 2 # used by FISH mask paper\n",
    "idx = 0\n",
    "sq_grads_expect = []\n",
    "sq_grads_expect_orig = {i: np.zeros(p.shape) for i, p in enumerate(parameters)}\n",
    "parameters = tuple(net.parameters())\n",
    "\n",
    "for test_batch, test_labels, idxs in test_loader:\n",
    " \n",
    "    \n",
    "    test_batch, test_labels = test_batch, test_labels\n",
    "    outputs, e1 = net(test_batch)\n",
    "    print('Outputs: ',outputs.shape)\n",
    "    _, preds = torch.max(outputs, 1)         \n",
    "    probs = F.softmax(outputs, dim=1).to('cpu')\n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    N, C = log_probs.shape\n",
    "    ps = probs.detach()\n",
    "    f = log_probs*torch.sqrt(ps)\n",
    "\n",
    "    for c in range(f.shape[1]):\n",
    "        grad_outs = torch.zeros_like(f)\n",
    "        grad_outs[:, c] = 1\n",
    "        grad = torch.autograd.grad(f, parameters, grad_outputs=grad_outs, retain_graph=True)\n",
    "        #Below if-else used to add tuples element-wise\n",
    "        if c == 0:\n",
    "            grad_list = [torch.square(g) for g in grad]\n",
    "        else:\n",
    "            grad_list = list( map(add, grad_list, [torch.square(g) for g in grad]) )\n",
    "        \n",
    "        net.zero_grad()\n",
    "    \n",
    "    grad_list = [np.array(i/N) for i in grad_list]\n",
    "    #Below if-else used to add tuples element-wise\n",
    "    if idx == 0:\n",
    "        sq_grads_expect = grad_list\n",
    "    else:\n",
    "        sq_grads_expect = list( map(add, sq_grads_expect, grad_list) )\n",
    "\n",
    "    print(\"original way next\")\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            grad_list_orig = torch.autograd.grad(log_probs[n][c], parameters, retain_graph=True)\n",
    "            for i, grad in enumerate(grad_list_orig):    # different layers\n",
    "                gsq = torch.square(grad).to('cpu') * probs[n][c] / N\n",
    "                sq_grads_expect_orig[i] += gsq.detach().numpy() # sq_grads_expect[i] + gsq\n",
    "                del gsq\n",
    "            net.zero_grad()\n",
    "    \n",
    "    idx += 1\n",
    "    if idx >= num_samples:\n",
    "        break\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet.ResNet18(num_classes=4)\n",
    "parameters = tuple(net.parameters())\n",
    "X_tr, Y_tr, X_te, Y_te = get_dataset('CIFAR10', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel, params, buffers = make_functional_with_buffers(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PER-SAMPLE-GRADIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a simple CNN and loss function:\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "num_models = 10\n",
    "batch_size = 64\n",
    "data = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "\n",
    "targets = torch.randint(10, (64,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device=device)\n",
    "predictions = model(data) # move the entire mini-batch through the model\n",
    "\n",
    "loss = loss_fn(predictions, targets)\n",
    "loss.backward() # back propogate the 'average' gradient of this mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(sample, target):\n",
    "    \n",
    "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
    "    target = target.unsqueeze(0)\n",
    "\n",
    "    prediction = model(sample)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    return torch.autograd.grad(loss, list(model.parameters()))\n",
    "\n",
    "\n",
    "def compute_sample_grads(data, targets):\n",
    "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
    "    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n",
    "    sample_grads = zip(*sample_grads)\n",
    "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
    "    return sample_grads\n",
    "\n",
    "per_sample_grads = compute_sample_grads(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(per_sample_grads[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "\n",
    "fmodel, params, buffers = make_functional_with_buffers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_stateless_model (params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = fmodel(params, buffers, batch) \n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_grad = grad(compute_loss_stateless_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n",
    "\n",
    "# we can double check that the results using functorch grad and vmap match the results of hand processing each one individually:\n",
    "for per_sample_grad, ft_per_sample_grad in zip(per_sample_grads, ft_per_sample_grads):\n",
    "    assert torch.allclose(per_sample_grad, ft_per_sample_grad, atol=3e-3, rtol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
